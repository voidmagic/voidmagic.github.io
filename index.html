<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Qian Wang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Qian Wang">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Qian Wang">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Qian Wang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Qian Wang" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Qian Wang</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-fairseq-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/12/fairseq-1/" class="article-date">
  <time datetime="2020-10-12T14:45:05.000Z" itemprop="datePublished">2020-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/12/fairseq-1/">Fairseq漫游指南（1）——命令行工具</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>2017年9月，Facebook AI Research开源了序列建模工具Fairseq。作为对Lua/Torch版本的改进，新款Fairseq基于Python和Pytorch，更加简单易用人性化。经过三年的迭代，fairseq目前已经拥有近两百位contributor，总代码量7万余行，功能和规模都已不同往日。</p>
<p>作为一个通用的序列建模工具，fairseq可以在多个自然语言处理任务上使用，如机器翻译、自动摘要、语音识别等文本生成任务，或者BERT、GPT等语言模型的训练；同时fairseq还实现了目前常用的多数模型，如RNN、CNN、Transformer、RoBERTa、XLM等。除了大量内置的任务和模型，fairseq还提供了极为简洁的接口，以便于使用者扩展已有模型、验证新的想法。</p>
<p>本文将以训练Transformer-based机器翻译模型为例，介绍fairseq的基本使用方法。</p>
<hr>
<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>深度神经网络模型的训练需要GPU支持，因此硬件方面需要安装有NVIDIA GPU的服务器，这里以GTX1080（驱动版本430.64，CUDA版本10.1）的Ubuntu 16.04为例，其他GPU、驱动、操作系统可能有细微差异。为了保证环境的一致性，在环境搭建中将从pytorch的安装开始。</p>
<ol>
<li>安装pytorch</li>
</ol>
<p>使用conda安装pytorch的时候，可以同时指定CUDA版本，意思是安装使用指定CUDA预编译的pytorch：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">conda</span> create -n fairseq python=<span class="number">3</span>.<span class="number">7</span></span><br><span class="line"><span class="attribute">conda</span> install pytorch=<span class="number">1</span>.<span class="number">6</span> torchvision cudatoolkit=<span class="number">10</span>.<span class="number">1</span> -c pytorch -y </span><br></pre></td></tr></table></figure>

<ol start="2">
<li>安装fairseq</li>
</ol>
<p>由于直接使用pip安装的fairseq版本（0.9.0）还停留在2019年12月，为了使用更新的特性，我们选择GitHub上的最新版本（commit 77983ee）：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https:<span class="regexp">//gi</span>thub.com<span class="regexp">/pytorch/</span>fairseq.git</span><br><span class="line">cd fairseq &amp;&amp; git checkout <span class="number">522</span>c76b &amp;&amp; pip install --editable ./</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>安装apex（可选）</li>
</ol>
<p>Apex是NVIDIA为Pytorch开发的混合精度训练库，在多卡训练、半精度训练的过程中可以带来更快的训练效率。安装apex的时候需要注意，由于安装过程会编译CUDA代码，且需要与pytorch使用同一版本的CUDA编译，因此要先安装与pytorch一致的CUDA。下面的命令会下载10.1版本的CUDA，并将其安装在用户目录下（不需要管理员权限），之后用其编译安装apex：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="regexp">//</span>developer.nvidia.com<span class="regexp">/compute/</span>cuda<span class="regexp">/10.1/</span>Prod<span class="regexp">/local_installers/</span>cuda_10.<span class="number">1.105</span>_418.<span class="number">39</span>_linux.run</span><br><span class="line">mkdir <span class="variable">$HOME</span><span class="regexp">/.cuda &amp;&amp; sh cuda_10.1.105_418.39_linux.run --silent --toolkit --toolkitpath=$HOME/</span>.cuda --defaultroot=<span class="variable">$HOME</span>/.cuda</span><br><span class="line">export CUDA_HOME=<span class="variable">$HOME</span>/.cuda</span><br><span class="line">git clone https:<span class="regexp">//gi</span>thub.com<span class="regexp">/NVIDIA/</span>apex</span><br><span class="line">cd apex &amp;&amp; pip install -v --no-cache-dir --global-option=<span class="string">&quot;--cpp_ext&quot;</span> --global-option=<span class="string">&quot;--cuda_ext&quot;</span> ./</span><br></pre></td></tr></table></figure>


<ol start="4">
<li>验证安装</li>
</ol>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">python</span> <span class="selector-tag">-c</span> &quot;<span class="selector-tag">import</span> <span class="selector-tag">torch</span>;<span class="selector-tag">print</span>(<span class="selector-tag">torch</span><span class="selector-class">.__version__</span>, <span class="selector-tag">torch</span><span class="selector-class">.version</span><span class="selector-class">.cuda</span>)&quot;</span><br></pre></td></tr></table></figure>
<p>会显示pytorch版本（1.6.0）和对应cuda版本（10.1）；</p>
<figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -<span class="keyword">c</span> <span class="string">&quot;import fairseq;print(fairseq.__version__)&quot;</span></span><br></pre></td></tr></table></figure>
<p>会显示fairseq版本（0.9.0）；</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">python</span> <span class="selector-tag">-c</span> &quot;<span class="selector-tag">import</span> <span class="selector-tag">fairseq</span>;<span class="selector-tag">print</span>(<span class="selector-tag">fairseq</span><span class="selector-class">.utils</span><span class="selector-class">.multi_tensor_l2norm_available</span>)&quot;</span><br></pre></td></tr></table></figure>
<p>会显示apex是否成功安装（True）。</p>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>在机器翻译中，需要双语平行数据来进行模型的训练，在这里使用fairseq中提供的数据：<br><code>bash fairseq/examples/translation/prepare-iwslt14.sh</code></p>
<p>这个脚本会下载IWSLT 14 英语和德语的平行数据，并进行分词、BPE等操作，处理的结果为：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span></span><br><span class="line">├── code</span><br><span class="line">├── <span class="keyword">test</span>.<span class="keyword">de</span></span><br><span class="line">├── <span class="keyword">test</span>.<span class="keyword">en</span></span><br><span class="line">├── tmp</span><br><span class="line">├── train.<span class="keyword">de</span></span><br><span class="line">├── train.<span class="keyword">en</span></span><br><span class="line">├── valid.<span class="keyword">de</span></span><br><span class="line">└── valid.<span class="keyword">en</span></span><br></pre></td></tr></table></figure>


<h2 id="数据二进制化"><a href="#数据二进制化" class="headerlink" title="数据二进制化"></a>数据二进制化</h2><p>之后，使用fairseq的预处理命令fairseq-preprocess将文本数据转换为二进制的文件：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fairseq-preprocess --source-lang <span class="keyword">de</span> --target-lang <span class="keyword">en</span> \</span><br><span class="line">    --trainpref iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span>/train \</span><br><span class="line">    --validpref iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span>/valid \</span><br><span class="line">    --testpref iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span>/<span class="keyword">test</span></span><br></pre></td></tr></table></figure>
<p>其中，<code>trainpref</code>、<code>validpref</code>和<code>testpref</code>代表两个语言对应文件的前缀（路径和文件名的前缀），<code>source-lang</code>和<code>target-lang</code>两个参数代表两个语言对应文件的后缀名（不代表具体语言，只是通过后缀区分两种语言的数据），fairseq通过这几个参数的组合，来寻找对应的文本数据。</p>
<p>例如我们的数据中只有训练数据和测试数据，且文件后缀为<code>src</code>和<code>tgt</code>，即<code>train.src</code>、<code>train.tgt</code>、<code>test.src</code>和<code>test.tgt</code>，那么通过指定<code>--source-lang src --target-lang tgt --trainpref train --testpref test</code>，也可以读取的对应的文件。</p>
<p>预处理命令首先会从训练文本数据中构建词表。在默认情况下，会将所有出现过的单词根据词频排序，并将这个排序后的单词列表作为最终的词表。同时，fairseq还提供了相关的参数来自定义词表：</p>
<p><code>--thresholdsrc/--thresholdtgt</code>，分别对应源端（source）和目标端（target）的词表的最低词频，词频低于这个阈值的单词将不会出现在词表中，而是统一使用一个unknown标签来代替。</p>
<p><code>--srcdict/--tgtdict</code>，其参数为一个文件名，即使用已有的词表，而不去根据文本数据中单词的词频构建词表。已有的词表文件中，每一行包含一个单词及其词频（这个词频只作为排序和阈值过滤的依据，不代表实际的词频）。</p>
<p><code>--nwordssrc/--nwordstgt</code>，源端和目标端词表的大小，在对单词根据词频排序后，取前n个词来构建词表，剩余的单词使用一个统一的unknown标签代替。</p>
<p><code>--joined-dictionary</code>，源端和目标端使用同一个词表，对于相似语言（如英语和西班牙语）来说，有很多的单词是相同的，使用同一个词表可以降低词表和参数的总规模。</p>
<p>构建的词表是一个单词和序号之间的一对一映射，这个序号是单词在词表中的下标位置。预处理命令在构建词表之后，会将文本数据转换为数值形式，也就是把文本中的每一个词，转换为对应的序号。之后，数值化的文本数据会被进一步编码，默认情况下使用Memory-Mapped IndexedDataset，这种数据编码方式不仅可以压缩文件大小，还可以根据索引进行随机读取，因此在训练的时候不需要加载全部数据，从而节约内存使用。</p>
<p>二进制化的数据文件会默认保存在data-bin目录下，包括生成的词表、训练数据、验证数据和测试数据。也可以通过–destdir参数，将生成的数据保存在其他目录。</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>在对数据进行预处理之后，就可以开始训练翻译模型了。模型训练使用的命令是fairseq-train，在参数中需要指定训练数据、模型、优化器等参数：</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">fairseq</span><span class="literal">-</span><span class="comment">train</span> <span class="comment">data</span><span class="literal">-</span><span class="comment">bin</span> --<span class="comment">arch</span> <span class="comment">transformer_iwslt_de_en</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">max</span><span class="literal">-</span><span class="comment">tokens</span> <span class="comment">4096</span> --<span class="comment">max</span><span class="literal">-</span><span class="comment">update</span> <span class="comment">30000</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">optimizer</span> <span class="comment">adam</span> --<span class="comment">lr</span><span class="literal">-</span><span class="comment">scheduler</span> <span class="comment">inverse_sqrt</span> --<span class="comment">lr</span> <span class="comment">0</span><span class="string">.</span><span class="comment">0007</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">criterion</span> <span class="comment">label_smoothed_cross_entropy</span> --<span class="comment">label</span><span class="literal">-</span><span class="comment">smoothing</span> <span class="comment">0</span><span class="string">.</span><span class="comment">1</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">no</span><span class="literal">-</span><span class="comment">progress</span><span class="literal">-</span><span class="comment">bar</span> --<span class="comment">save</span><span class="literal">-</span><span class="comment">interval</span><span class="literal">-</span><span class="comment">updates</span> <span class="comment">1000</span> </span><br></pre></td></tr></table></figure>

<p>fairseq-train提供了大量的训练参数，从而进行定制化的训练过程，其中主要的参数可以分为数据（data）、模型（model）、优化（optimizing）、训练（分布式和多GPU等）、日志（log）和模型保存（checkpointing）等。</p>
<ol>
<li>数据部分</li>
</ol>
<p>数据部分的常用参数主要有训练数据的位置（路径），训练时的batch size等。其中，batch size可以通过两种方法指定，<code>--max-tokens</code>是按照词的数量来分的batch，比如<code>--max-tokens 4096</code>指每个batch中包含4096个词；另外还可以通过句子来指定，如<code>--max-sentences 128</code>指每个batch中包含128个句子。</p>
<ol start="2">
<li>模型部分</li>
</ol>
<p>模型部分的参数主要有<code>--arch</code>，用指定所使用的网络模型结构，有大量预设的可以选择。其命名一般为“model_setting”，如“transformer_iwslt_de_en”就是使用Transformer模型和预设的iwslt_de_en超参数。在大数据上进行训练的时候，可以选择“transformer_wmt_en_de”、“transformer_wmt_en_de_big”等设置。除了Transformer之外，还有LSTM、FConv等模型及对应的预设超参数可以选择。</p>
<p>在使用预设模型的同时，还可以通过额外的命令行参数来覆盖已有的参数，如“transformer_wmt_en_de”中预设的编码器层数是6，可以通过<code>--encoder-layers 12</code>将编码器改为12层。</p>
<ol start="3">
<li>优化部分</li>
</ol>
<p>通过<code>--criterion</code>可以指定使用的损失函数，如cross_entropy等。和<code>--arch</code>参数一样，也可以通过命令行参数来覆盖特定损失的默认参数，比如通过<code>--label-smoothing</code>0.1，可以将label_smoothed_cross_entropy损失中默认为0的label-smoothing值改为0.1。</p>
<p>通过<code>--optimizer</code>可以指定所使用的优化器，如adam、sgd等；通过<code>--lr-scheduler</code>可以指定学习率缩减的方式。</p>
<p>通常来说，参数优化紧跟梯度计算，即每计算一次梯度，就会进行一次参数更新。在某些时候，我们希望在多次梯度计算之后进行一次更新，来模拟多GPU训练，可以通过–update-freq来指定，比如在单个GPU上指定“–update-freq 4”来训练，结果和4个GPU训练是基本等价的。</p>
<ol start="4">
<li>训练部分</li>
</ol>
<p>Fairseq支持单GPU、多GPU、多机器等多种训练方式，在默认情况下，会根据当前机器的GPU数量来确定训练方式。在绝大多数情况下，这部分参数都不需要关心，而是通过系统环境变量的方式，<code>export CUDA_VISIBLE_DEVICES=0,1</code>,来指定单卡、多卡训练。</p>
<p>如果所使用的GPU支持半精度，那么可以通过参数<code>--fp16</code>进行混合精度训练，可以极大提高模型训练的速度。通过<code>torch.cuda.get_device_capability(0)[0]</code>可以确定GPU是否支持半精度，如果该值小于7则不支持，大于等于7则支持。</p>
<ol start="5">
<li>日志和模型保存</li>
</ol>
<p>在默认情况下，fairseq使用tqdm和进度条来展示训练过程，但是这种方法不适合长时间在后台进行模型训练。通过<code>--no-progress-bar</code>参数可以改为逐行打印日志，方便保存。默认情况下，每训练100步之后会打印一次，通过<code>--log-interval</code>数可以进行修改。<br>Fairseq在训练过程中会保存中间模型，保存的位置可以通过–save-dir指定，其默认为checkpoints。中间模型保存的频率有两种指定方式，<code>--save-interval</code>指定了每N个epoch（遍历训练数据N次）保存一次；<code>--save-interval-updates</code>指定了每N步保存一次，这种通过step来保存模型的方法目前更为常用。<br>Note：在使用多GPU训练时，指定的batch size（max tokens或max sentences）是单个GPU上的数量，以token计算为例，最终batch size的大小为max-tokens、GPU数量、update-freq的乘积。</p>
<h2 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h2><p>在经过了充分训练之后，就可以使用模型来进行翻译了。Fairseq提供了两种解码的方式：批生成解码（fairseq-generate）和交互式解码（fairseq-interactive）。</p>
<ol>
<li>fairseq-generate</li>
</ol>
<p>fairseq-generate用来解码之前经过预处理（fairseq-preprocess）的数据：</p>
<figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fairseq-<span class="keyword">generate</span> data-bin --path checkpoints/checkpoint_best<span class="variable">.pt</span> --remove-bpe</span><br></pre></td></tr></table></figure>

<p>默认情况下，这个命令会从预处理的数据中，解码测试数据（test set）。通过—gen-subset可以指定解码其他部分，如<code>--gen-subset train</code>就会翻译整个训练数据。<br>如果不想得到翻译结果，只想看到翻译结果的BLEU分值，可以通过—quiet参数，只显示翻译进度和最后打分。</p>
<p>通过<code>--beam</code>、<code>--lenpen</code>和<code>--unkpen</code>，可以分别设置beam search中的beam size，长度惩罚和unk惩罚。</p>
<p>参数<code>--remove-bpe</code>可以指定对翻译结果的后处理，由于在准备数据的时候，使用了BPE切分，该参数会把BPE切分的词合并为完整的单词。如果不加该参数，则输出的翻译结果和BLEU打分都是按照未合并BPE进行的。如果准备数据时BPE切分使用的是sentencepiece（<a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece%EF%BC%89%EF%BC%8C%E9%82%A3%E4%B9%88%E5%8F%82%E6%95%B0%E7%9A%84%E5%80%BC%E8%BF%98%E5%8F%AF%E4%BB%A5%E8%AE%BE%E4%B8%BA%60--remove-bpe">https://github.com/google/sentencepiece），那么参数的值还可以设为`--remove-bpe</a> sentencepiece`，以合并sentencepiece的切分。</p>
<ol start="2">
<li>fairseq-interactive</li>
</ol>
<p>fairseq-interactive可以进行交互式逐句解码，其参数和fairseq-generate基本一致。以下命令用来逐行翻译test.de文件中的句子：</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">cat</span> test.de | fairseq-interactive <span class="class"><span class="keyword">data</span>-bin <span class="comment">--path checkpoints/checkpoint_best.pt --remove-bpe</span></span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>保存翻译结果</li>
</ol>
<p>在默认情况下，两种解码方式会将翻译结果直接显示出来，如果想保存翻译结果，可以通过<code>--results-path</code>参数来指定保存结果的位置，或者可以通过重定向的方法将输出保存到文件：</p>
<figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fairseq-<span class="keyword">generate</span> someargs &gt; result<span class="variable">.txt</span></span><br></pre></td></tr></table></figure>

<p>这两种方法得到的翻译结果文件，包含了解码日志（log）、原文、译文、打分等信息，并且顺序与原文不一致，通过以下命令可以得到排序后的译文：</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">grep</span> ^H result.txt | <span class="keyword">sort</span> -n -k <span class="number">2</span> -t <span class="string">&#x27;-&#x27;</span> | cut -f <span class="number">3</span></span><br></pre></td></tr></table></figure>


<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文以训练一个简单翻译模型为例，介绍了fairseq几个命令行工具（fairseq-preprocess、train、generate、interactive）的基本使用方法。通过命令行调用的方法，可以在不接触内部代码的情况下，完成训练解码的整个流程。</p>
<p>在后续的文章中，将会陆续介绍fairseq的扩展方法，比如定义新的任务，实现新的模型等。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/10/12/fairseq-1/" data-id="ckg69g0v6000046qr3h9sflwl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-fairseq-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/10/12/fairseq-2/" class="article-date">
  <time datetime="2020-10-12T14:45:05.000Z" itemprop="datePublished">2020-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/10/12/fairseq-2/">Fairseq漫游指南（2）——扩展模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文为Fairseq漫游指南系列的第二篇文章。前面一篇文章以基于Transformer的翻译模型为例，对Fairseq的命令行使用方法进行了初步的介绍。Fairseq预设了大量的任务和模型，可以根据需要准备数据，并参考对应任务、模型的参数进行训练和解码。</p>
<p>在实际的使用中，现有的模型可能无法满足真实任务的需要，我们可能需要处理不同类型的输入输出，或者需要对模型进行修改以验证新的想法。在这种情况下，只通过命令行调用预设任务和模型的方法就存在很大的局限，我们需要对Fairseq本身进行扩展，以满足实际多样化的需求。</p>
<p>本文以实现一个可以双向翻译（EN-DE和DE-EN）的Transformer模型为例，来介绍Fairseq扩展的使用方法。</p>
<h2 id="Fairseq扩展概述"><a href="#Fairseq扩展概述" class="headerlink" title="Fairseq扩展概述"></a>Fairseq扩展概述</h2><p>Fairseq允许用户在不修改源代码的情况下，以插件的形式进行扩展。目前，可以自定义五种插件：<br>    1. 任务（Tasks）：任务定义了我们要完成的整个流程，包括读取数据组成batch、模型初始化、训练、测试等。<br>    2. 模型（Models）：模型定义了网络的结构、包含的参数、前向计算过程。<br>    3. 评价准则（Criterions）：评价准则也就是损失函数，用来根据网络输出和真实标签计算损失。<br>    4. 优化器（Optimizers）：在反向传播之后，优化器决定了更新模型参数的方式。<br>    5. 学习率调度器（Learning Rate Schedulers）：学习率调度器可以用来根据训练过的步数，动态调整学习率。</p>
<p>对于这五种插件，Fairseq自身的代码中提供了大量的预设，可以在对应的目录下查看，如<code>fairseq/models</code>目录下提供了多种模型的实现。在指定了这五种插件（可以为预设值，也可以为用户编写的插件）之后，fairseq的训练流程可以抽象为：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    itr = task.get<span class="constructor">_batch_iterator(<span class="params">task</span>.<span class="params">dataset</span>(&#x27;<span class="params">train</span>&#x27;)</span>)</span><br><span class="line">    <span class="keyword">for</span> num_updates, batch <span class="keyword">in</span> enumerate(itr):</span><br><span class="line">        task.train<span class="constructor">_step(<span class="params">batch</span>, <span class="params">model</span>, <span class="params">criterion</span>, <span class="params">optimizer</span>)</span></span><br><span class="line">        average<span class="constructor">_and_clip_gradients()</span></span><br><span class="line">        optimizer.step<span class="literal">()</span></span><br><span class="line">        lr_scheduler.step<span class="constructor">_update(<span class="params">num_updates</span>)</span></span><br><span class="line">    lr_scheduler.step(epoch)</span><br></pre></td></tr></table></figure>

<p>如前所述，模型的单步训练过程在任务中定义，即<code>task.train_step</code>。默认情况下，其实现如下：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def train<span class="constructor">_step(<span class="params">self</span>, <span class="params">batch</span>, <span class="params">model</span>, <span class="params">criterion</span>, <span class="params">optimizer</span>, <span class="operator">**</span><span class="params">unused</span>)</span>:</span><br><span class="line">    loss = criterion(model, batch)</span><br><span class="line">    optimizer.backward(loss)</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<p>只通过命令行的方式，可以选择使用不同的预设插件，如LSTM、Transformer等不同的模型。但如果我们想要扩展Fairseq没有提供的一些功能，那么就需要我们自己编写一些插件，并进行注册，以便Fairseq在运行的时候可以加载我们自定义的插件。接下来我们以一个最简单的例子，来实现自己的Transformer模型。</p>
<p>首先需要建立我们的代码仓库，假设代码存放在<code>$HOME/codebase/custom</code>：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">├── custom</span><br><span class="line">    └── <span class="module-access"><span class="module"><span class="identifier">__init__</span>.</span></span>py</span><br></pre></td></tr></table></figure>

<p>其中，<code>__init__.py</code>的内容如下：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fairseq.models.transformer <span class="keyword">import</span> TransformerModel, transformer_iwslt_de_en</span><br><span class="line"><span class="keyword">from</span> fairseq.models <span class="keyword">import</span> register_model, register_model_architecture</span><br><span class="line"></span><br><span class="line">@register_model(<span class="string">&#x27;my_transformer&#x27;</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="symbol">MyTransformer</span>(<span class="symbol">TransformerModel</span>):</span><br><span class="line">    <span class="symbol">pass</span></span><br><span class="line"></span><br><span class="line">@<span class="symbol">register_model_architecture</span>(&#x27;<span class="symbol">my_transformer</span>&#x27;, &#x27;<span class="symbol">iwslt_arch</span>&#x27;)</span><br><span class="line"><span class="symbol">def</span> <span class="symbol">my_transformer_iwslt</span>(<span class="symbol">args</span>):</span><br><span class="line">    <span class="symbol">transformer_iwslt_de_en</span>(<span class="symbol">args</span>)</span><br></pre></td></tr></table></figure>

<p>在Fairseq中，模型称为<code>model</code>，模型对应的超参数称为<code>model_architecture</code>。在这个例子中，我们定义了一个名为<code>my_transformer</code>的模型，以及其对应的<code>iwslt_arch</code>超参数。由于模型直接继承了预设的<code>TransformerModel</code>，超参数直接调用了<code>transformer_iwslt_de_en</code>，因此其功能没有任何的改变，只是名字发生了改变。在编写了这个简单的插件后，就可以通过命令行来进行调用了：</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">fairseq</span><span class="literal">-</span><span class="comment">train</span> <span class="comment">data</span><span class="literal">-</span><span class="comment">bin</span> --<span class="comment">arch</span> <span class="comment">iwslt_arch</span> --<span class="comment">user</span><span class="literal">-</span><span class="comment">dir</span> <span class="comment">$HOME/codebase/custom</span> --<span class="comment">max</span><span class="literal">-</span><span class="comment">tokens</span> <span class="comment">4096</span> --<span class="comment">optimizer</span> <span class="comment">adam</span></span><br></pre></td></tr></table></figure>

<p>其中，<code>data-bin</code>是上一篇文章”命令行工具“中预处理的数据路径。该命令可以在任何目录下执行，只要通过<code>--user-dir $HOME/codebase/custom</code>参数指定我们的插件代码位置即可。</p>
<p>从上面的例子可以看出，自定义并使用一个模型插件需要以下几个步骤：<br>    1. 创建一个python module，即包含<code>__init__.py</code>文件的目录（这个例子中为<code>$HOME/codebase/custom</code>）；<br>    2. 定义新的模型类（类名可以任意，只要不和其他重复即可），并用<code>@register_model(&#39;model_name&#39;)</code>装饰器来进行注册（model_name即模型名，Fairseq通过这个名字来定位插件对应的类）；<br>    3. 定义模型对应的预设超参数model_architecture，这是一个函数，接收<code>args</code>参数。比如想将dropout预设为0.1，可以通过<code>args.dropout = 0.1</code>来完成。和模型类似，想要Fairseq能够将其识别为预设超参数，需要使用<code>@register_model_architecture(&#39;model_name&#39;, &#39;arch_name&#39;)</code>来进行注册，其中<code>model_name</code>是模型名，<code>arch_name</code>是预设值的名字；<br>    4. 如果插件的实现在<code>__init__.py</code>之外的文件中，那么还需要在<code>__init__.py</code>文件中导入注册的model和model_architecture，这是因为fairseq在运行时通过查找已经导入（加载）的插件名（如模型名）来定位具体的实现，如果不进行导入，那么即便指定了<code>--user-dir</code>，fairseq也只能加载在<code>__init__.py</code>中的代码，而找不到在其他文件中定义的插件。在这个例子中，由于model和model_architecture都定义在了<code>__init__.py</code>文件中，因此不需要额外的导入；<br>    5. 在命令行调用的时候，指定<code>--user-dir</code>参数为插件路径，并使用<code>--arch</code>来告诉Fairseq使用我们自定义的模型和超参数。</p>
<p>定义新的任务、优化器等，和定义新的模型基本一致，都是通过定义一个新的类，并通过<code>@register_*</code>来注册。下面，我们将实现一个双向翻译、参数共享的翻译系统，来看一下扩展在实际中如何使用。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们使用和系列第一篇《命令行工具》中一致的环境：<br>    1. python 3.7<br>    2. pytorch 1.6.0<br>    3. Fairseq，commit 522c76b<br>    4. cuda 10.1<br>    5. Apex 0.1</p>
<p>对于数据，我们同样使用iwslt 14英德平行数据来进行训练。由于我们的目的是进行两种语言的双向翻译，编码器和解码器都需要拥有处理两种语言的能力，因此我们需要对两种语言使用共享的词表，在fairseq的预处理命令中，可以通过<code>--joined-dictionary</code>参数来指定：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bash fairseq/examples/translation/prepare-iwslt14.<span class="keyword">sh</span></span><br><span class="line"></span><br><span class="line">fairseq-preprocess --source-lang <span class="keyword">de</span> --target-lang <span class="keyword">en</span> \</span><br><span class="line">    --trainpref iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span>/train \</span><br><span class="line">    --validpref iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span>/valid \</span><br><span class="line">    --testpref iwslt14.tokenized.<span class="keyword">de</span>-<span class="keyword">en</span>/<span class="keyword">test</span> --joined-dictionary</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>默认情况下，预处理后的二进制数据文件保存在data-bin目录下。</p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>对于双向翻译任务，我们希望给定一个源语言的句子，模型能解码出一个目标语言的句子；给定一个目标语言的句子，模型能够解码出一个源语言的句子。为了达到这个目的，我们需要模型能够区分出输入是哪种语言，或者说，希望翻译为哪种语言。在多语言机器翻译中，一个简单而有效的做法是，在输入的句子前面加上一个标签来指明希望模型输出的语言，比如在句子前面加一个<code>__2&lt;en&gt;__</code>，来告诉模型我们希望得到英文的翻译结果。</p>
<p>为了给输入句子加上标签，我们需要在读取数据和组成batch之间进行处理，即读取所有句对，给句对的源端部分加上指明目标语言的标签，再根据句长，将相似长度的句子打包为一个batch，并将这个batch数值化，来构成模型的输入。如前所述，读取数据组成batch的操作需要在Task中进行，因此我们需要自定义一个Task，来对数据进行处理。</p>
<p>在模型部分，我们希望编码器和解码器共享自注意力和前馈神经网络中的参数，即Transformer中self attention和feed forward模块的参数。这一部分的改变在模型中体现，因此我们还需要自定义一个基于Transformer的Model，以实现参数的共享。</p>
<p>在明确了目标之后，我们首先需要创建代码库，保存在<code>codebase/custom</code>目录下：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">└── custom</span><br><span class="line">    ├── bidirectional_transformer.<span class="keyword">py</span></span><br><span class="line">    ├── bidirectional_translation_task.<span class="keyword">py</span></span><br><span class="line">    └── __init__.<span class="keyword">py</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span> directory, <span class="number">3</span> <span class="keyword">files</span></span><br></pre></td></tr></table></figure>

<p>其中，<code>bidirectional_transformer.py</code>保存我们自定义的模型，<code>bidirectional_translation_task.py</code>保存我们自定义的任务。为了使Fairseq能够加载自定义模型和任务，需要在<code>__init__.py</code>中将其导入：</p>
<figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> (</span><br><span class="line">    bidirectional_transformer <span class="keyword">as</span> _,</span><br><span class="line">    bidirectional_translation_task <span class="keyword">as</span> _,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>接下来，我们的目标就是实现<code>bidirectional_transformer</code>和<code>bidirectional_translation_task</code>了。</p>
<h2 id="参数共享的模型"><a href="#参数共享的模型" class="headerlink" title="参数共享的模型"></a>参数共享的模型</h2><p>模型部分相对比较简单，由于Fairseq中实现了大量的预设模型，因此我们在实现自定义模型的时候，应该尽量复用已有的代码，通过模型类的继承、方法的重载来实现功能上的修改和扩展。我们直接使用Transformer的实现，并在模型初始化之后，指定参数共享的部分：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from fairseq.models.transformer import TransformerModel, transformer_iwslt_de_en</span><br><span class="line">from fairseq.models import register_model, register_model_architecture</span><br><span class="line"></span><br><span class="line">@register<span class="constructor">_model(&#x27;<span class="params">bidirectional_transformer</span>&#x27;)</span></span><br><span class="line"><span class="keyword">class</span> <span class="constructor">BidirectionalTransformerModel(TransformerModel)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>, <span class="params">args</span>, <span class="params">encoder</span>, <span class="params">decoder</span>)</span>:</span><br><span class="line">        super<span class="literal">()</span>.<span class="constructor">__init__(<span class="params">args</span>, <span class="params">encoder</span>, <span class="params">decoder</span>)</span></span><br><span class="line">        self.make<span class="constructor">_shared_component()</span></span><br><span class="line">    </span><br><span class="line">    def make<span class="constructor">_shared_component(<span class="params">self</span>)</span>:</span><br><span class="line">        <span class="keyword">for</span> enc_layer, dec_layer <span class="keyword">in</span> zip(self.encoder.layers, self.decoder.layers):</span><br><span class="line">            dec_layer.self_attn.k_proj = enc_layer.self_attn.k_proj</span><br><span class="line">            dec_layer.self_attn.v_proj = enc_layer.self_attn.v_proj</span><br><span class="line">            dec_layer.self_attn.q_proj = enc_layer.self_attn.q_proj</span><br><span class="line">            dec_layer.self_attn.out_proj = enc_layer.self_attn.out_proj</span><br><span class="line">            dec_layer.fc1 = enc_layer.fc1</span><br><span class="line">            dec_layer.fc2 = enc_layer.fc2</span><br><span class="line"></span><br><span class="line">@register<span class="constructor">_model_architecture(&#x27;<span class="params">bidirectional_transformer</span>&#x27;, &#x27;<span class="params">iwslt_arch</span>&#x27;)</span></span><br><span class="line">def iwslt<span class="constructor">_preset_hyperparameters(<span class="params">args</span>)</span>:</span><br><span class="line">    transformer<span class="constructor">_iwslt_de_en(<span class="params">args</span>)</span></span><br></pre></td></tr></table></figure>

<p>通过继承Fairseq中的<code>Transformer</code>模型，我们的<code>BidirectionalTransformerModel</code>就可以实现与Transformer相同的功能。在模型的实例化方法<code>__init__</code>中，首先调用父类<code>TransformerModel</code>的初始化方法，来初始化模型及其参数，然后调用<code>make_shared_component</code>方法，来共享编码器和解码器每一层中的<code>self_attn</code>和<code>fc1</code>、<code>fc2</code>参数。同时，我们使用了<code>transformer_iwslt_de_en</code>来定义名为<code>iwslt_arch</code>的预设超参数。最后通过<code>register_model</code>和<code>register_model_architecture</code>来注册模型，就可以在Fairseq中使用了。</p>
<h2 id="双向翻译任务"><a href="#双向翻译任务" class="headerlink" title="双向翻译任务"></a>双向翻译任务</h2><p>在自定义的双向翻译任务中，我们需要将标签加到每个源端句子前面。由于我们的目的和翻译任务基本一致，因此可以复用Fairseq中的<code>TranslationTask</code>，只需要实现数据加载部分即可。完整代码如下：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from fairseq.tasks import register_task</span><br><span class="line">from fairseq.tasks.translation import TranslationTask</span><br><span class="line">from fairseq.data import data_utils, PrependTokenDataset, LanguagePairDataset, ConcatDataset</span><br><span class="line"></span><br><span class="line">@register_task(<span class="string">&#x27;bidirectional_translation_task&#x27;</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BidirectionalTranslationTask</span>(<span class="title">TranslationTask</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(<span class="keyword">self</span>, split, **kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        shared_dict = <span class="keyword">self</span>.src_dict</span><br><span class="line">        src, tgt = data_utils.infer_language_pair(<span class="keyword">self</span>.args.data)</span><br><span class="line">        prefix = os.path.join(<span class="keyword">self</span>.args.data, <span class="string">&#x27;&#123;&#125;.&#123;&#125;-&#123;&#125;.&#x27;</span>.format(split, src, tgt))</span><br><span class="line"></span><br><span class="line">        src_raw_dataset = data_utils.load_indexed_dataset(prefix + <span class="keyword">self</span>.args.source_lang, shared_dict)</span><br><span class="line">        tgt_raw_dataset = data_utils.load_indexed_dataset(prefix + <span class="keyword">self</span>.args.target_lang, shared_dict)</span><br><span class="line"></span><br><span class="line">        src_prepend_dataset = PrependTokenDataset(src_raw_dataset, shared_dict.index(<span class="string">&#x27;__2&lt;&#123;&#125;&gt;__&#x27;</span>.format(<span class="keyword">self</span>.args.target_lang)))</span><br><span class="line">        tgt_prepend_dataset = PrependTokenDataset(tgt_raw_dataset, shared_dict.index(<span class="string">&#x27;__2&lt;&#123;&#125;&gt;__&#x27;</span>.format(<span class="keyword">self</span>.args.source_lang)))</span><br><span class="line"></span><br><span class="line">        src_dataset = src_prepend_dataset <span class="keyword">if</span> split == <span class="string">&#x27;test&#x27;</span> <span class="keyword">else</span> ConcatDataset([src_prepend_dataset, tgt_prepend_dataset])</span><br><span class="line">        tgt_dataset = tgt_raw_dataset     <span class="keyword">if</span> split == <span class="string">&#x27;test&#x27;</span> <span class="keyword">else</span> ConcatDataset([tgt_raw_dataset,     src_raw_dataset])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.datasets[split] = LanguagePairDataset(</span><br><span class="line">            src_dataset, src_dataset.sizes, shared_dict, tgt_dataset, tgt_dataset.sizes, shared_dict)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup_task</span><span class="params">(cls, args, **kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        task = <span class="keyword">super</span>(BidirectionalTranslationTask, cls).setup_task(args)</span><br><span class="line">        <span class="keyword">for</span> lang_token <span class="keyword">in</span> sorted([<span class="string">&#x27;__2&lt;&#123;&#125;&gt;__&#x27;</span>.format(args.source_lang), <span class="string">&#x27;__2&lt;&#123;&#125;&gt;__&#x27;</span>.format(args.target_lang)])<span class="symbol">:</span></span><br><span class="line">            task.src_dict.add_symbol(lang_token)</span><br><span class="line">            task.tgt_dict.add_symbol(lang_token)</span><br><span class="line">        <span class="keyword">return</span> task</span><br></pre></td></tr></table></figure>

<p>参考<code>fairseq/tasks/translation.py</code>的代码可以看到，数据加载实在方法<code>load_dataset</code>中完成的，我们可以在其基础上（加载源语言到目标语言的数据），增加目标语言到源语言数据的加载，并给加载的数据添加标签。<code>load_dataset</code>方法的基本流程是，通过<code>spilt</code>参数，来加载对应的数据，并将加载的数据赋值给<code>self.datasets[split]</code>。其中<code>split</code>参数一般为<code>train</code>、<code>valid</code>或者<code>test</code>。默认情况下，训练、验证、解码分别使用对应的数据，但也可以通过命令行来指定，如<code>fairseq-generate --gen-subset train</code>就会解码训练数据（即split为train）。</p>
<p>在我们的实现中，读取数据和添加标签的流程如下：<br>    1. 仿照<code>fairseq/tasks/translation.py</code>中的代码，使用<code>data_utils.load_indexed_dataset</code>来分别读取两种语言预处理后的二进制数据；<br>    2. 使用<code>PrependTokenDataset</code>给两种语言的数据都创建一个加标签的版本；<br>    3. 如果是测试的情况下<code>split == &#39;test&#39;</code>，只使用 <code>src_prepend_dataset</code>和<code>tgt_raw_dataset</code>来构建数据集；如果是训练或者验证，则将加标签的源语言和目标语言数据使用<code>ConcatDataset</code>进行拼接，得到<code>src_dataset</code>，将两种语言不加标签的数据拼接，得到<code>tgt_dataset</code>，来构建数据集；<br>    4. 根据<code>src_dataset</code>和<code>tgt_dataset</code>，创建一个<code>LanguagePairDataset</code>，并赋值给<code>self.datasets[split]</code>。</p>
<p>在这个例子中，我们使用到了<code>PrependTokenDataset</code>、<code>LanguagePairDataset</code>、<code>ConcatDataset</code>三个Fairseq中定义的类来完成加标签、拼接数据等操作。在<code>fairseq/data</code>目录下，还有大量预定义的数据类可供使用，同时，我们还可以继承预定义的类来扩展其功能，完成更复杂的数据处理。</p>
<p>最后，由于我们使用了额外的标签来指定目标语言，所以需要在词表中添加对应的语言标签。通过查看<code>TranslationTask</code>的代码可知，词表的创建和初始化是在<code>setup_task</code>中进行的，我们通过重写该方法，在任务创建完成后，为<code>src_dict</code>和<code>tgt_dict</code>分别添加源语言标签和目标语言标签。</p>
<h2 id="训练和解码"><a href="#训练和解码" class="headerlink" title="训练和解码"></a>训练和解码</h2><p>在创建了自定义的任务和模型后，就可以使用该插件来进行训练了。进行训练和解码的命令和前文所介绍的基本一致，只需要指定插件代码的位置<code>--user-dir</code>、模型结构<code>--arch</code>和任务<code>--task</code>：</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">fairseq</span><span class="literal">-</span><span class="comment">train</span> <span class="comment">data</span><span class="literal">-</span><span class="comment">bin</span> --<span class="comment">max</span><span class="literal">-</span><span class="comment">tokens</span> <span class="comment">4096</span> --<span class="comment">max</span><span class="literal">-</span><span class="comment">update</span> <span class="comment">50000</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">arch</span> <span class="comment">iwslt_arch</span> --<span class="comment">task</span> <span class="comment">bidirectional_translation_task</span> --<span class="comment">user</span><span class="literal">-</span><span class="comment">dir</span> <span class="comment">$HOME/codebase/custom</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">optimizer</span> <span class="comment">adam</span> --<span class="comment">lr</span><span class="literal">-</span><span class="comment">scheduler</span> <span class="comment">inverse_sqrt</span> --<span class="comment">lr</span> <span class="comment">0</span><span class="string">.</span><span class="comment">0007</span> <span class="comment">\</span></span><br><span class="line"><span class="comment"></span>        --<span class="comment">criterion</span> <span class="comment">label_smoothed_cross_entropy</span> --<span class="comment">label</span><span class="literal">-</span><span class="comment">smoothing</span> <span class="comment">0</span><span class="string">.</span><span class="comment">1</span></span><br></pre></td></tr></table></figure>

<p>解码的命令不需要指定模型结构：</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">fairseq</span><span class="literal">-</span><span class="comment">generate</span> <span class="comment">data</span><span class="literal">-</span><span class="comment">bin</span> --<span class="comment">path</span> <span class="comment">checkpoints/checkpoint_best</span><span class="string">.</span><span class="comment">pt</span> --<span class="comment">remove</span><span class="literal">-</span><span class="comment">bpe</span> --<span class="comment">user</span><span class="literal">-</span><span class="comment">dir</span> <span class="comment">$HOME/codebase/custom</span> --<span class="comment">task</span> <span class="comment">bidirectional_translation_task</span> --<span class="comment">source</span><span class="literal">-</span><span class="comment">lang</span> <span class="comment">en</span> --<span class="comment">target</span><span class="literal">-</span><span class="comment">lang</span> <span class="comment">de</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">fairseq</span><span class="literal">-</span><span class="comment">generate</span> <span class="comment">data</span><span class="literal">-</span><span class="comment">bin</span> --<span class="comment">path</span> <span class="comment">checkpoints/checkpoint_best</span><span class="string">.</span><span class="comment">pt</span> --<span class="comment">remove</span><span class="literal">-</span><span class="comment">bpe</span> --<span class="comment">user</span><span class="literal">-</span><span class="comment">dir</span> <span class="comment">$HOME/codebase/custom</span> --<span class="comment">task</span> <span class="comment">bidirectional_translation_task</span> --<span class="comment">source</span><span class="literal">-</span><span class="comment">lang</span> <span class="comment">de</span> --<span class="comment">target</span><span class="literal">-</span><span class="comment">lang</span> <span class="comment">en</span></span><br></pre></td></tr></table></figure>

<p>其中，参数<code>--source-lang</code>和<code>--target-lang</code>可以进行特定方向的翻译，用来验证模型训练得到的双向翻译能力。如果不指定这两个参数，则默认是和数据预处理时相同的翻译方向（德语到英语）。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文通过一个双向翻译的例子，介绍了Fairseq扩展插件的基本使用方法。大多数的NLP任务都可以在不修改源码的情况下，通过编写插件来实现，这在很大程度上简化了实验的流程，我们只需要编写插件实现与原方法、模型不同的部分，而不需要关注重复的模式和训练流程。</p>
<p>在实际开发插件的过程中，关键的问题在于如何定位我们需要修改的部分，以及如何最大程度地复用Fairseq已经实现的部分。后续文章将介绍Fairseq中已经实现的一些任务、模型，以及数据集等常用的工具，以便了解我们要实现的功能在fairseq中是否已经有对应的实现及实现对应的位置。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/10/12/fairseq-2/" data-id="ckg69g0vj000146qr3sys3dbk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/10/12/fairseq-1/">Fairseq漫游指南（1）——命令行工具</a>
          </li>
        
          <li>
            <a href="/2020/10/12/fairseq-2/">Fairseq漫游指南（2）——扩展模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Qian Wang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>